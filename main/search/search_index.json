{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>TODO: add badges</p>"},{"location":"#overview","title":"Overview","text":"<p>TODO</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>analora</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>analora</code> to a new version will possibly break any code that was using the old version of <code>analora</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>analora</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"refs/metric/","title":"Metric","text":""},{"location":"refs/metric/#analora.metric","title":"analora.metric","text":"<p>Contain functions to compute metrics.</p>"},{"location":"refs/metric/#analora.metric.accuracy","title":"analora.metric.accuracy","text":"<pre><code>accuracy(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the accuracy metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import accuracy\n&gt;&gt;&gt; accuracy(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'accuracy': 1.0, 'count_correct': 5, 'count_incorrect': 0, 'count': 5, 'error': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.average_precision","title":"analora.metric.average_precision","text":"<pre><code>average_precision(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the average precision metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import average_precision\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; metrics = average_precision(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1])\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': 1.0, 'count': 5}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; metrics = average_precision(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_score=np.array([2, -1, 0, 3, 1]),\n...     label_type=\"binary\",\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': 1.0, 'count': 5}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; metrics = average_precision(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.3, 0.5],\n...             [0.4, 0.4, 0.2],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n...     label_type=\"multiclass\",\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': array([0.833..., 0.75 , 0.75 ]),\n 'count': 6,\n 'macro_average_precision': 0.777...,\n 'micro_average_precision': 0.75,\n 'weighted_average_precision': 0.777...}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; metrics = average_precision(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, -1], [-1, 1, 2], [0, 2, 3], [3, -2, -4], [1, -3, -5]]),\n...     label_type=\"multilabel\",\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': array([1. , 1. , 0.477...]),\n 'count': 5,\n 'macro_average_precision': 0.825...,\n 'micro_average_precision': 0.588...,\n 'weighted_average_precision': 0.804...}\n</code></pre>"},{"location":"refs/metric/#analora.metric.balanced_accuracy","title":"analora.metric.balanced_accuracy","text":"<pre><code>balanced_accuracy(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the accuracy metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import balanced_accuracy\n&gt;&gt;&gt; balanced_accuracy(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'balanced_accuracy': 1.0, 'count': 5}\n</code></pre>"},{"location":"refs/metric/#analora.metric.binary_average_precision","title":"analora.metric.binary_average_precision","text":"<pre><code>binary_average_precision(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the average precision metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, *)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import binary_average_precision\n&gt;&gt;&gt; metrics = binary_average_precision(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1])\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': 1.0, 'count': 5}\n</code></pre>"},{"location":"refs/metric/#analora.metric.binary_confusion_matrix","title":"analora.metric.binary_confusion_matrix","text":"<pre><code>binary_confusion_matrix(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the confusion matrix metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import binary_confusion_matrix\n&gt;&gt;&gt; binary_confusion_matrix(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n{'confusion_matrix': array([[2, 0], [0, 3]]),\n 'count': 5,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 3}\n</code></pre>"},{"location":"refs/metric/#analora.metric.binary_fbeta_score","title":"analora.metric.binary_fbeta_score","text":"<pre><code>binary_fbeta_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    betas: Sequence[float] = (1,),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the F-beta metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1,)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import binary_fbeta_score\n&gt;&gt;&gt; binary_fbeta_score(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n... )\n{'count': 5, 'f1': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.binary_jaccard","title":"analora.metric.binary_jaccard","text":"<pre><code>binary_jaccard(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Jaccard metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import binary_jaccard\n&gt;&gt;&gt; binary_jaccard(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'jaccard': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.binary_precision","title":"analora.metric.binary_precision","text":"<pre><code>binary_precision(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the precision metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import binary_precision\n&gt;&gt;&gt; binary_precision(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'precision': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.binary_recall","title":"analora.metric.binary_recall","text":"<pre><code>binary_recall(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the recall metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import binary_recall\n&gt;&gt;&gt; binary_recall(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'recall': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.binary_roc_auc","title":"analora.metric.binary_roc_auc","text":"<pre><code>binary_roc_auc(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p>"},{"location":"refs/metric/#analora.metric.binary_top_k_accuracy","title":"analora.metric.binary_top_k_accuracy","text":"<pre><code>binary_top_k_accuracy(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    k: Sequence[int] = (2,),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Area Under the Top-k Accuracy classification metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>k</code> <code>Sequence[int]</code> <p>The numbers of most likely outcomes considered to find the correct label.</p> <code>(2,)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p>"},{"location":"refs/metric/#analora.metric.confusion_matrix","title":"analora.metric.confusion_matrix","text":"<pre><code>confusion_matrix(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the confusion matrix metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import confusion_matrix\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; confusion_matrix(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'confusion_matrix': array([[2, 0], [0, 3]]),\n 'count': 5,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 3}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; confusion_matrix(\n...     y_true=np.array([0, 1, 1, 2, 2, 2]),\n...     y_pred=np.array([0, 1, 1, 2, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'confusion_matrix': array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]), 'count': 6}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; confusion_matrix(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'confusion_matrix': array([[[2, 0], [0, 3]],\n                            [[3, 0], [0, 2]],\n                            [[2, 0], [0, 3]]]),\n 'count': 5}\n</code></pre>"},{"location":"refs/metric/#analora.metric.energy_distance","title":"analora.metric.energy_distance","text":"<pre><code>energy_distance(\n    u_values: ndarray,\n    v_values: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the energy distance between two 1D distributions.</p> <p>Parameters:</p> Name Type Description Default <code>u_values</code> <code>ndarray</code> <p>The values observed in the (empirical) distribution.</p> required <code>v_values</code> <code>ndarray</code> <p>The values observed in the (empirical) distribution.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import energy_distance\n&gt;&gt;&gt; energy_distance(u_values=np.array([1, 2, 3, 4, 5]), v_values=np.array([1, 2, 3, 4, 5]))\n{'count': 5, 'energy_distance': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.fbeta_score","title":"analora.metric.fbeta_score","text":"<pre><code>fbeta_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    betas: Sequence[float] = (1,),\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the F-beta metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1,)</code> <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import fbeta_score\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; fbeta_score(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'f1': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; fbeta_score(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'count': 5, 'f1': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; fbeta_score(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'count': 6,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; fbeta_score(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'count': 5,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.jaccard","title":"analora.metric.jaccard","text":"<pre><code>jaccard(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Jaccard metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import jaccard\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; jaccard(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'jaccard': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; jaccard(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'count': 5, 'jaccard': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; jaccard(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'count': 6,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; jaccard(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'count': 5,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.jensen_shannon_divergence","title":"analora.metric.jensen_shannon_divergence","text":"<pre><code>jensen_shannon_divergence(\n    p: ndarray,\n    q: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Jensen-Shannon (JS) divergence between two distributions.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The true probability distribution.</p> required <code>q</code> <code>ndarray</code> <p>The model probability distribution.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import jensen_shannon_divergence\n&gt;&gt;&gt; jensen_shannon_divergence(\n...     p=np.array([0.1, 0.6, 0.1, 0.2]), q=np.array([0.2, 0.5, 0.2, 0.1])\n... )\n{'size': 4, 'jensen_shannon_divergence': 0.027...}\n</code></pre>"},{"location":"refs/metric/#analora.metric.kl_div","title":"analora.metric.kl_div","text":"<pre><code>kl_div(\n    p: ndarray,\n    q: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Kullback-Leibler (KL) divergence between two distributions.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The true probability distribution.</p> required <code>q</code> <code>ndarray</code> <p>The model probability distribution.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import kl_div\n&gt;&gt;&gt; kl_div(p=np.array([0.1, 0.6, 0.1, 0.2]), q=np.array([0.2, 0.5, 0.2, 0.1]))\n{'size': 4, 'kl_pq': 0.109..., 'kl_qp': 0.116...}\n</code></pre>"},{"location":"refs/metric/#analora.metric.mean_absolute_error","title":"analora.metric.mean_absolute_error","text":"<pre><code>mean_absolute_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean absolute error (MAE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import mean_absolute_error\n&gt;&gt;&gt; mean_absolute_error(y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5]))\n{'count': 5, 'mean_absolute_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.mean_absolute_percentage_error","title":"analora.metric.mean_absolute_percentage_error","text":"<pre><code>mean_absolute_percentage_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean absolute percentage error (MAPE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import mean_absolute_percentage_error\n&gt;&gt;&gt; mean_absolute_percentage_error(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'mean_absolute_percentage_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.mean_squared_error","title":"analora.metric.mean_squared_error","text":"<pre><code>mean_squared_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean squared error (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import mean_squared_error\n&gt;&gt;&gt; mean_squared_error(y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5]))\n{'count': 5, 'mean_squared_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.mean_squared_log_error","title":"analora.metric.mean_squared_log_error","text":"<pre><code>mean_squared_log_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean squared logarithmic error (MSLE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import mean_squared_log_error\n&gt;&gt;&gt; mean_squared_log_error(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'mean_squared_log_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.mean_tweedie_deviance","title":"analora.metric.mean_tweedie_deviance","text":"<pre><code>mean_tweedie_deviance(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    powers: Sequence[float] = (0,),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean Tweedie deviance regression loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>powers</code> <code>Sequence[float]</code> <p>The Tweedie power parameter. The higher power the less weight is given to extreme deviations between true and predicted targets.</p> <code>(0,)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import mean_tweedie_deviance\n&gt;&gt;&gt; mean_tweedie_deviance(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'mean_tweedie_deviance_power_0': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.median_absolute_error","title":"analora.metric.median_absolute_error","text":"<pre><code>median_absolute_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the median absolute error.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import median_absolute_error\n&gt;&gt;&gt; median_absolute_error(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'median_absolute_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multiclass_average_precision","title":"analora.metric.multiclass_average_precision","text":"<pre><code>multiclass_average_precision(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the average precision metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multiclass_average_precision\n&gt;&gt;&gt; metrics = multiclass_average_precision(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.3, 0.5],\n...             [0.4, 0.4, 0.2],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': array([0.833..., 0.75 , 0.75 ]),\n 'count': 6,\n 'macro_average_precision': 0.777...,\n 'micro_average_precision': 0.75,\n 'weighted_average_precision': 0.777...}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multiclass_confusion_matrix","title":"analora.metric.multiclass_confusion_matrix","text":"<pre><code>multiclass_confusion_matrix(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the confusion matrix metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multiclass_confusion_matrix\n&gt;&gt;&gt; multiclass_confusion_matrix(\n...     y_true=np.array([0, 1, 1, 2, 2, 2]), y_pred=np.array([0, 1, 1, 2, 2, 2])\n... )\n{'confusion_matrix': array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]), 'count': 6}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multiclass_fbeta_score","title":"analora.metric.multiclass_fbeta_score","text":"<pre><code>multiclass_fbeta_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    betas: Sequence[float] = (1,),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the F-beta metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1,)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multiclass_fbeta_score\n&gt;&gt;&gt; multiclass_fbeta_score(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n... )\n{'count': 6,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multiclass_jaccard","title":"analora.metric.multiclass_jaccard","text":"<pre><code>multiclass_jaccard(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Jaccard metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multiclass_jaccard\n&gt;&gt;&gt; multiclass_jaccard(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]), y_pred=np.array([0, 0, 1, 1, 2, 2])\n... )\n{'count': 6,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multiclass_precision","title":"analora.metric.multiclass_precision","text":"<pre><code>multiclass_precision(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the precision metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multiclass_precision\n&gt;&gt;&gt; multiclass_precision(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]), y_pred=np.array([0, 0, 1, 1, 2, 2])\n... )\n{'count': 6,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multiclass_recall","title":"analora.metric.multiclass_recall","text":"<pre><code>multiclass_recall(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the recall metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multiclass_recall\n&gt;&gt;&gt; multiclass_recall(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]), y_pred=np.array([0, 0, 1, 1, 2, 2])\n... )\n{'count': 6,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multiclass_roc_auc","title":"analora.metric.multiclass_roc_auc","text":"<pre><code>multiclass_roc_auc(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p>"},{"location":"refs/metric/#analora.metric.multiclass_top_k_accuracy","title":"analora.metric.multiclass_top_k_accuracy","text":"<pre><code>multiclass_top_k_accuracy(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    k: Sequence[int] = (2,),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Area Under the Top-k Accuracy classification metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>k</code> <code>Sequence[int]</code> <p>The numbers of most likely outcomes considered to find the correct label.</p> <code>(2,)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p>"},{"location":"refs/metric/#analora.metric.multilabel_average_precision","title":"analora.metric.multilabel_average_precision","text":"<pre><code>multilabel_average_precision(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the average precision metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multilabel_average_precision\n&gt;&gt;&gt; metrics = multilabel_average_precision(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, -1], [-1, 1, 2], [0, 2, 3], [3, -2, -4], [1, -3, -5]]),\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': array([1. , 1. , 0.477...]),\n 'count': 5,\n 'macro_average_precision': 0.825...,\n 'micro_average_precision': 0.588...,\n 'weighted_average_precision': 0.804...}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multilabel_confusion_matrix","title":"analora.metric.multilabel_confusion_matrix","text":"<pre><code>multilabel_confusion_matrix(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the confusion matrix metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multilabel_confusion_matrix\n&gt;&gt;&gt; multilabel_confusion_matrix(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'confusion_matrix': array([[[2, 0], [0, 3]],\n                            [[3, 0], [0, 2]],\n                            [[2, 0], [0, 3]]]),\n 'count': 5}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multilabel_fbeta_score","title":"analora.metric.multilabel_fbeta_score","text":"<pre><code>multilabel_fbeta_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    betas: Sequence[float] = (1,),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the F-beta metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1,)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multilabel_fbeta_score\n&gt;&gt;&gt; multilabel_fbeta_score(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'count': 5,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multilabel_jaccard","title":"analora.metric.multilabel_jaccard","text":"<pre><code>multilabel_jaccard(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Jaccard metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multilabel_jaccard\n&gt;&gt;&gt; multilabel_jaccard(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'count': 5,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multilabel_precision","title":"analora.metric.multilabel_precision","text":"<pre><code>multilabel_precision(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the precision metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multilabel_precision\n&gt;&gt;&gt; multilabel_precision(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'count': 5,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multilabel_recall","title":"analora.metric.multilabel_recall","text":"<pre><code>multilabel_recall(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the recall metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import multilabel_recall\n&gt;&gt;&gt; multilabel_recall(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'count': 5,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.multilabel_roc_auc","title":"analora.metric.multilabel_roc_auc","text":"<pre><code>multilabel_roc_auc(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p>"},{"location":"refs/metric/#analora.metric.ndcg","title":"analora.metric.ndcg","text":"<pre><code>ndcg(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    k: int | None = None,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Normalized Discounted Cumulative Gain (NDCG) metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target targets of multilabel classification, or true scores of entities to be ranked. Negative values in y_true may result in an output that is not between 0 and 1. This input must be an array of shape <code>(n_samples, n_labels)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The predicted scores, can either be probability estimates, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_labels)</code>.</p> required <code>k</code> <code>int | None</code> <p>Only consider the highest <code>k</code> scores in the ranking. If <code>None</code>, use all outputs.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import ndcg\n&gt;&gt;&gt; ndcg(\n...     y_true=np.array([[1, 0, 0], [1, 2, 0], [1, 1, 2], [0, 0, 1]]),\n...     y_score=np.array(\n...         [[2.0, 1.0, 0.0], [0.0, 1.0, -1.0], [0.0, 0.0, 1.0], [1.0, 2.0, 3.0]]\n...     ),\n... )\n{'count': 4, 'ndcg': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.pearsonr","title":"analora.metric.pearsonr","text":"<pre><code>pearsonr(\n    x: ndarray,\n    y: ndarray,\n    *,\n    alternative: str = \"two-sided\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Pearson correlation coefficient and p-value for testing non-correlation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The first input array.</p> required <code>y</code> <code>ndarray</code> <p>The second input array.</p> required <code>alternative</code> <code>str</code> <p>The alternative hypothesis. Default is 'two-sided'. The following options are available: - 'two-sided': the correlation is nonzero - 'less': the correlation is negative (less than zero) - 'greater': the correlation is positive (greater than zero)</p> <code>'two-sided'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import pearsonr\n&gt;&gt;&gt; pearsonr(x=np.array([1, 2, 3, 4, 5]), y=np.array([1, 2, 3, 4, 5]))\n{'count': 5, 'pearson_coeff': 1.0, 'pearson_pvalue': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.precision","title":"analora.metric.precision","text":"<pre><code>precision(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the precision metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import precision\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; precision(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'precision': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; precision(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'count': 5, 'precision': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; precision(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'count': 6,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; precision(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'count': 5,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.r2_score","title":"analora.metric.r2_score","text":"<pre><code>r2_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the R^2 (coefficient of determination) regression score metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import r2_score\n&gt;&gt;&gt; r2_score(y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5]))\n{'count': 5, 'r2_score': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.recall","title":"analora.metric.recall","text":"<pre><code>recall(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import recall\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; recall(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'recall': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; recall(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'count': 5, 'recall': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; recall(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'count': 6,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; recall(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'count': 5,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.regression_errors","title":"analora.metric.regression_errors","text":"<pre><code>regression_errors(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the regression error metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import regression_errors\n&gt;&gt;&gt; regression_errors(y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5]))\n{'count': 5,\n 'mean_absolute_error': 0.0,\n 'median_absolute_error': 0.0,\n 'mean_squared_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.roc_auc","title":"analora.metric.roc_auc","text":"<pre><code>roc_auc(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import roc_auc\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; metrics = roc_auc(y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1]))\n&gt;&gt;&gt; metrics\n{'count': 5, 'roc_auc': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; metrics = roc_auc(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_score=np.array([2, -1, 0, 3, 1]),\n...     label_type=\"binary\",\n... )\n&gt;&gt;&gt; metrics\n{'count': 5, 'roc_auc': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; metrics = roc_auc(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.3, 0.5],\n...             [0.4, 0.4, 0.2],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n...     label_type=\"multiclass\",\n... )\n&gt;&gt;&gt; metrics\n{'count': 6,\n 'macro_roc_auc': 0.833...,\n 'micro_roc_auc': 0.826...,\n 'roc_auc': array([0.9375, 0.8125, 0.75  ]),\n 'weighted_roc_auc': 0.833...}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; metrics = roc_auc(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, -1], [-1, 1, 2], [0, 2, 3], [3, -2, -4], [1, -3, -5]]),\n...     label_type=\"multilabel\",\n... )\n&gt;&gt;&gt; metrics\n{'count': 5,\n 'macro_roc_auc': 0.666...,\n 'micro_roc_auc': 0.544...,\n 'roc_auc': array([1., 1., 0.]),\n 'weighted_roc_auc': 0.625}\n</code></pre>"},{"location":"refs/metric/#analora.metric.root_mean_squared_error","title":"analora.metric.root_mean_squared_error","text":"<pre><code>root_mean_squared_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the root mean squared error (RMSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import root_mean_squared_error\n&gt;&gt;&gt; root_mean_squared_error(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'root_mean_squared_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.spearmanr","title":"analora.metric.spearmanr","text":"<pre><code>spearmanr(\n    x: ndarray,\n    y: ndarray,\n    *,\n    alternative: str = \"two-sided\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Spearman correlation coefficient and p-value for testing non-correlation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The first input array.</p> required <code>y</code> <code>ndarray</code> <p>The second input array.</p> required <code>alternative</code> <code>str</code> <p>The alternative hypothesis. Default is 'two-sided'. The following options are available: - 'two-sided': the correlation is nonzero - 'less': the correlation is negative (less than zero) - 'greater': the correlation is positive (greater than zero)</p> <code>'two-sided'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import spearmanr\n&gt;&gt;&gt; spearmanr(\n...     x=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n...     y=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n... )\n{'count': 9, 'spearman_coeff': 1.0, 'spearman_pvalue': 0.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.top_k_accuracy","title":"analora.metric.top_k_accuracy","text":"<pre><code>top_k_accuracy(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    k: Sequence[int] = (2,),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Area Under the Top-k Accuracy classification metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. The binary case expects scores with shape <code>(n_samples,)</code> while the multiclass case expects scores with shape <code>(n_samples, n_classes)</code>.</p> required <code>k</code> <code>Sequence[int]</code> <p>The numbers of most likely outcomes considered to find the correct label.</p> <code>(2,)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import top_k_accuracy\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; metrics = top_k_accuracy(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1]), k=[1, 2]\n... )\n&gt;&gt;&gt; metrics\n{'count': 5, 'top_1_accuracy': 1.0, 'top_2_accuracy': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; metrics = top_k_accuracy(\n...     y_true=np.array([0, 1, 2, 2]),\n...     y_score=np.array(\n...         [[0.5, 0.2, 0.2], [0.3, 0.4, 0.2], [0.2, 0.4, 0.3], [0.7, 0.2, 0.1]]\n...     ),\n...     k=[1, 2, 3],\n... )\n&gt;&gt;&gt; metrics\n{'count': 4, 'top_1_accuracy': 0.5, 'top_2_accuracy': 0.75, 'top_3_accuracy': 1.0}\n</code></pre>"},{"location":"refs/metric/#analora.metric.wasserstein_distance","title":"analora.metric.wasserstein_distance","text":"<pre><code>wasserstein_distance(\n    u_values: ndarray,\n    v_values: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    nan_policy: str = \"propagate\"\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Wasserstein distance between two 1D discrete distributions.</p> <p>Parameters:</p> Name Type Description Default <code>u_values</code> <code>ndarray</code> <p>An array that contains a sample from a probability distribution or the support (set of all possible values) of a probability distribution. Each element is an observation or possible value.</p> required <code>v_values</code> <code>ndarray</code> <p>An array that contains a sample from or the support of a second distribution.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input arrays. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from analora.metric import wasserstein_distance\n&gt;&gt;&gt; wasserstein_distance(\n...     u_values=np.array([1, 2, 3, 4, 5]), v_values=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'wasserstein_distance': 0.0}\n</code></pre>"}]}